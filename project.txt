Project GOGGA: A Sovereign Bicameral AI Architecture for the South African Digital Ecosystem1. Executive Summary: The Case for a Localized Cognitive EngineThe democratization of Artificial Intelligence has largely been driven by hyperscale providers located in the Northern Hemisphere, creating a distinct latency and cultural gap for applications deployed in the Global South. South Africa, with its unique blend of first-world financial infrastructure and developing-world connectivity challenges, requires a fundamentally different approach to AI application design. "Project GOGGA"â€”named after the colloquial South African term for an insect, symbolizing agility, ubiquity, and a connection to the local soilâ€”represents a reference architecture for a sovereign, localized chat application designed to bridge this gap.This comprehensive technical report details the implementation of a high-performance, cost-optimized conversational AI platform tailored specifically for the South African market. The architecture moves beyond the monolithic model approach, adopting a "Bicameral" cognitive strategy that routes traffic between two distinct neural processing layers: a "Speed Layer" powered by Cerebras Systemsâ€™ wafer-scale inference technology, and a "Complex Layer" utilizing the massive Qwen 3 235B parameter model. This bifurcation is not merely an optimization; it is a necessity for economic viability in a currency-volatile environment like South Africa, where the exchange rate between the Rand (ZAR) and the US Dollar significantly impacts operational expenditure.The system is engineered with a modern stack comprising a FastAPI backend for high-concurrency asynchronous processing, a Next.js frontend capable of handling voice-first interactions (crucial for vernacular language users), and a robust integration with PayFast, South Africaâ€™s leading payment gateway, to handle recurring subscriptions. By leveraging Azure Container Apps in the southafricanorth region, the architecture ensures data sovereignty and minimal network latency, addressing the twin challenges of compliance (POPIA) and user experience.The following analysis provides an exhaustive breakdown of every architectural decision, code module, and configuration setting required to bring GOGGA to life, supported by rigorous cost modeling and performance data.2. The Bicameral Routing Architecture2.1 The Economic and Latency ImperativeIn standard Large Language Model (LLM) deployments, a single "frontier" model (such as GPT-4 or Claude 3.5 Sonnet) typically handles all user queries. While operationally simple, this approach is economically inefficient. A user greeting ("Howzit?") costs the same in compute resources as a complex legal query regarding the Rental Housing Act. For a South African startup paying in ZAR but consuming APIs billed in USD, this inefficiency is fatal.Project GOGGA introduces a Bicameral Routing Engine. This logic layer sits at the edge of the backend and acts as a semantic traffic controller. It evaluates the "cognitive load" required for a request and routes it to one of two providers.Table 1: The Bicameral Model Tiering StrategyFeatureSpeed Layer (The Reflexive Mind)Complex Layer (The Analytical Mind)Primary ModelLlama 3.1 8B 1Qwen 3 235B Instruct 2Inference ProviderCerebras Inference CloudCerebras Inference CloudInference Speed~2,200 tokens/second 1~1,400 tokens/second 2Input Cost$0.10 per million tokens 1$0.60 per million tokens 2Output Cost$0.10 per million tokens 1$1.20 per million tokens 2Use CasesPhatic communication, greetings, simple definitions, UI navigation help.Legal analysis (POPIA), translation (Zulu/Xhosa), coding, complex reasoning.Context Window8,000 - 128,000 tokens32,000 - 131,000 tokens 22.2 Deep Dive: Cerebras and the Speed LayerThe Speed Layer utilizes the Cerebras Wafer-Scale Engine (WSE). Unlike traditional GPUs that require data to move between separate chips (creating memory bandwidth bottlenecks), the WSE is a single chip the size of a wafer. This allows for memory bandwidth and communication speeds orders of magnitude higher than GPU clusters. For GOGGA, this translates to an inference speed of roughly 2,200 tokens per second for the Llama 3.1 8B model.1This speed is transformative for South African users on mobile networks. High latency in 3G/4G/LTE/5G switching often compounds with API latency. By reducing API latency to near-zero (sub-200ms for complete responses), GOGGA compensates for network jitter, providing a snappy experience even in rural areas like the Eastern Cape or Limpopo.2.3 Deep Dive: Qwen 3 and the Complex LayerThe Complex Layer is powered by Qwen 3 235B, specifically the qwen-3-235b-a22b-instruct-2507 snapshot hosted by Cerebras.2 This model is a Mixture-of-Experts (MoE) architecture, which allows it to punch significantly above its weight class in terms of reasoning while maintaining reasonable inference costs.Why Qwen 3 for South Africa?Multilingual capabilities: Qwen models have demonstrated superior performance in low-resource languages compared to many Western-centric models. This is critical for supporting South Africa's 11 official languages, particularly isiZulu and isiXhosa, which have complex agglutinative morphologies.Context Window: With a context window extending up to 131k tokens 2, Qwen 3 can ingest entire South African legal acts (e.g., The Constitution, The Consumer Protection Act) into its working memory for Retrieval Augmented Generation (RAG) tasks without losing coherence.Cost-Performance Ratio: At $0.60 per million input tokens, it is significantly cheaper than comparable frontier models (often $2.50+), aligning with the project's ZAR-sensitive financial modeling.3. Backend Implementation: The FastAPI CoreThe backbone of Project GOGGA is a Python-based FastAPI application. Python 3.11+ is selected for its performance improvements in asynchronous operations. FastAPI is chosen over Flask or Django for its native support of asynchronous request handling (async/await), which is critical when managing multiple concurrent connections to AI inference providers and payment gateways. Furthermore, its deep integration with Pydantic ensures that data structuresâ€”particularly financial payloadsâ€”are rigorously validated before processing.3.1 Project Directory StructureThe project follows a "Clean Architecture" pattern, separating the domain logic (AI, Payments) from the transport layer (API endpoints).gogga-backend/â”œâ”€â”€ app/â”‚   â”œâ”€â”€ init.pyâ”‚   â”œâ”€â”€ main.py                     # Application entry point and middleware configurationâ”‚   â”œâ”€â”€ config.py                   # Environment variable management via Pydantic Settingsâ”‚   â”œâ”€â”€ core/â”‚   â”‚   â”œâ”€â”€ init.pyâ”‚   â”‚   â”œâ”€â”€ router.py               # The Bicameral Routing Logicâ”‚   â”‚   â”œâ”€â”€ security.py             # API Key validation and JWT handlingâ”‚   â”‚   â””â”€â”€ exceptions.py           # Custom exception handlersâ”‚   â”œâ”€â”€ models/â”‚   â”‚   â”œâ”€â”€ init.pyâ”‚   â”‚   â”œâ”€â”€ domain.py               # Pydantic models for API request/responseâ”‚   â”‚   â””â”€â”€ database.py             # SQLModel schemas for persistenceâ”‚   â”œâ”€â”€ services/â”‚   â”‚   â”œâ”€â”€ init.pyâ”‚   â”‚   â”œâ”€â”€ ai_service.py           # Cerebras SDK integrationâ”‚   â”‚   â”œâ”€â”€ cost_tracker.py         # Token accounting logicâ”‚   â”‚   â””â”€â”€ payfast_service.py      # Subscription management (MD5/PUT)â”‚   â””â”€â”€ api/â”‚       â”œâ”€â”€ init.pyâ”‚       â””â”€â”€ v1/â”‚           â”œâ”€â”€ endpoints/â”‚           â”‚   â”œâ”€â”€ chat.py         # Chat endpointsâ”‚           â”‚   â””â”€â”€ payments.py     # PayFast webhook and form generation endpointsâ”œâ”€â”€ tests/â”‚   â”œâ”€â”€ init.pyâ”‚   â”œâ”€â”€ test_routing.pyâ”‚   â””â”€â”€ test_payments.pyâ”œâ”€â”€ Dockerfile                      # Multi-stage build for Azureâ”œâ”€â”€ docker-compose.yml              # Local orchestrationâ”œâ”€â”€ requirements.txt                # Pinned dependenciesâ””â”€â”€.env.example                    # Template for environment variables3.2 Dependency Management and ConfigurationStability in production relies on pinned dependencies. The requirements.txt file must include the specific SDKs for Cerebras and the async drivers for database and HTTP interactions.requirements.txtfastapi>=0.109.0uvicorn[standard]>=0.27.0pydantic>=2.6.0pydantic-settings>=2.1.0cerebras_cloud_sdk>=1.0.0httpx>=0.26.0python-multipart>=0.0.9sqlmodel>=0.0.14python-dotenv>=1.0.1email-validator>=2.1.0asyncpg>=0.29.0alembic>=1.13.0The configuration module uses pydantic-settings to read from environment variables. This creates a strongly typed configuration object that fails fast at startup if critical keys (like the Cerebras API Key or PayFast Merchant ID) are missing. This is a crucial DevOps practice for Azure deployments to prevent runtime errors.app/config.pyPythonimport os
from pydantic_settings import BaseSettings
from pydantic import Field

class Settings(BaseSettings):
    PROJECT_NAME: str = "GOGGA API"
    API_V1_STR: str = "/api/v1"
    
    # Security
    SECRET_KEY: str = Field(..., env="SECRET_KEY")
    ACCESS_TOKEN_EXPIRE_MINUTES: int = 60 * 24 * 8  # 8 days
    
    # Cerebras Configuration
    CEREBRAS_API_KEY: str = Field(..., env="CEREBRAS_API_KEY")
    
    # Model Identification [1, 2]
    # Llama 3.1 8B for speed
    MODEL_SPEED: str = "llama3.1-8b"
    # Qwen 3 235B for reasoning and legal complexity
    MODEL_COMPLEX: str = "qwen-3-235b-a22b-instruct-2507"
    
    # Pricing Configuration (USD per Million Tokens) [1, 2]
    # These must be updated as Cerebras adjusts pricing
    COST_SPEED_INPUT: float = 0.10
    COST_SPEED_OUTPUT: float = 0.10
    COST_COMPLEX_INPUT: float = 0.60
    COST_COMPLEX_OUTPUT: float = 1.20
    
    # Exchange Rate (Manual or Live Feed)
    ZAR_USD_RATE: float = 18.50  # Example rate
    
    # PayFast Configuration
    PAYFAST_MERCHANT_ID: str = Field(..., env="PAYFAST_MERCHANT_ID")
    PAYFAST_MERCHANT_KEY: str = Field(..., env="PAYFAST_MERCHANT_KEY")
    PAYFAST_PASSPHRASE: str = Field(..., env="PAYFAST_PASSPHRASE")
    # 'sandbox' or 'production'
    PAYFAST_ENV: str = "sandbox"
    
    # Database
    DATABASE_URL: str = Field(..., env="DATABASE_URL")

    class Config:
        env_file = ".env"
        case_sensitive = True

settings = Settings()
3.3 The Intelligent Routing ServiceThe AIService class encapsulates the interaction with the Cerebras Cloud SDK. It implements the routing heuristic. In a mature system, this heuristic would be a small classifier model (like a BERT-Tiny or a logistic regression on embeddings). However, for the reference architecture, we implement a highly effective rule-based router that checks for keywords indicating legal/complex intent and query length.The service must also handle the system_prompt injection. The "persona" of GOGGA must be maintained across both models. However, the Qwen model (Complex Layer) requires a more detailed system prompt to constrain its massive knowledge base to the South African context (e.g., "Cite the South African Constitution, Chapter 2, Bill of Rights").app/services/ai_service.pyPythonimport time
import asyncio
from typing import Dict, Any, List, Optional
from cerebras.cloud.sdk import Cerebras
from app.config import settings
from app.services.cost_tracker import track_usage

# Initialize the Cerebras client
# The SDK automatically handles connection pooling
client = Cerebras(api_key=settings.CEREBRAS_API_KEY)

class AIService:
    
    @staticmethod
    def classify_intent(message: str) -> str:
        """
        The Bicameral Router.
        Determines whether to route to the Speed Layer (Llama 3.1 8B)
        or Complex Layer (Qwen 3 235B).
        
        Logic:
        1. Check for 'heavy' keywords (legal, coding, translation).
        2. Check for input length (long contexts require bigger models).
        3. Default to speed.
        """
        # Keywords that trigger the Qwen 3 235B model
        complex_keywords =
        
        message_lower = message.lower()
        
        # Check for complex keywords
        if any(keyword in message_lower for keyword in complex_keywords):
            return settings.MODEL_COMPLEX
            
        # Check for input length. 
        # Queries > 50 words often imply a need for detailed reasoning.
        if len(message.split()) > 50:
            return settings.MODEL_COMPLEX
            
        # Default to the Speed Layer
        return settings.MODEL_SPEED

    @staticmethod
    async def generate_response(
        user_id: str, 
        message: str, 
        history: Optional]] = None
    ) -> Dict[str, Any]:
        """
        Executes the inference request using the selected model.
        Tracks precise token usage and calculates cost.
        """
        model_id = AIService.classify_intent(message)
        start_time = time.time()
        
        # Construct the context window
        messages =
        
        # Base System Prompt
        base_system_prompt = (
            "You are Gogga, a helpful, witty, and knowledgeable South African AI assistant. "
            "You speak English but understand local slang (Mzansi style). "
            "Be concise and helpful."
        )
        
        # Augment system prompt for Complex Layer
        if model_id == settings.MODEL_COMPLEX:
            base_system_prompt += (
                " You are currently operating in 'Complex Mode'. "
                "You are an expert in South African law, advanced coding, and complex analysis. "
                "When discussing legal matters, cite relevant South African Acts (e.g., POPIA, CPA). "
                "Be precise and authoritative."
            )
            
        messages.append({"role": "system", "content": base_system_prompt})
        
        # Append history if provided (limit to last 5 turns to save context)
        if history:
            messages.extend(history[-5:])
            
        messages.append({"role": "user", "content": message})
        
        try:
            # We use to_thread to make the blocking SDK call async-compatible
            # This prevents blocking the FastAPI event loop
            response = await asyncio.to_thread(
                client.chat.completions.create,
                messages=messages,
                model=model_id,
                temperature=0.7,
                max_tokens=2048, # Generous limit for Qwen
                top_p=0.95
            )
            
            # Extract usage metrics
            usage = response.usage
            input_tokens = usage.prompt_tokens
            output_tokens = usage.completion_tokens
            content = response.choices.message.content
            
            # Calculate Latency
            latency = time.time() - start_time
            
            # Asynchronously track cost. We await this to ensure the ledger 
            # is updated before returning, or we could use BackgroundTasks.
            cost_data = await track_usage(
                user_id=user_id,
                model=model_id,
                input_tokens=input_tokens,
                output_tokens=output_tokens
            )
            
            return {
                "response": content,
                "meta": {
                    "model_used": model_id,
                    "latency_seconds": round(latency, 3),
                    "tokens": {"input": input_tokens, "output": output_tokens},
                    "cost_usd": cost_data["usd"],
                    "cost_zar": cost_data["zar"]
                }
            }
            
        except Exception as e:
            print(f"Inference Error: {e}")
            # Fallback logic could go here (e.g., try Speed layer if Complex fails)
            raise e
3.4 Precise Token Cost TrackingFinancial viability depends on accurate unit economics. The track_usage function does not merely log tokens; it calculates the precise micro-cost of every interaction based on the specific pricing tier of the model used. This data is essential for "unit economics" analysisâ€”determining if a subscription price covers the user's AI consumption.app/services/cost_tracker.pyPythonfrom app.config import settings

async def track_usage(user_id: str, model: str, input_tokens: int, output_tokens: int) -> Dict[str, float]:
    """
    Calculates the cost of an interaction based on the specific model pricing.
    Pricing is derived from Cerebras documentation.[1, 2]
    
    Returns:
        Dict containing cost in USD and ZAR.
    """
    input_cost_per_m = 0.0
    output_cost_per_m = 0.0
    
    # Determine Pricing Tier
    if model == settings.MODEL_SPEED:
        input_cost_per_m = settings.COST_SPEED_INPUT   # $0.10 / M
        output_cost_per_m = settings.COST_SPEED_OUTPUT # $0.10 / M
    elif model == settings.MODEL_COMPLEX:
        input_cost_per_m = settings.COST_COMPLEX_INPUT   # $0.60 / M
        output_cost_per_m = settings.COST_COMPLEX_OUTPUT # $1.20 / M
        
    # Calculate Cost (Input + Output)
    input_cost = (input_tokens / 1_000_000) * input_cost_per_m
    output_cost = (output_tokens / 1_000_000) * output_cost_per_m
    total_cost_usd = input_cost + output_cost
    
    # Convert to ZAR for local reporting
    total_cost_zar = total_cost_usd * settings.ZAR_USD_RATE
    
    # In a production environment, we would write this to a 'ledger' table
    # await db.execute("INSERT INTO token_ledger...")
    
    print(f" User: {user_id} | Model: {model} | Input: {input_tokens} | Output: {output_tokens} | Cost: ${total_cost_usd:.6f}")
    
    return {
        "usd": total_cost_usd,
        "zar": total_cost_zar
    }
4. Fintech Integration: PayFast Recurring BillingIntegrating with PayFast (Network International) is a critical requirement for monetizing GOGGA. Unlike Stripe or PayPal, PayFast has specific idiosyncrasies regarding signature generation and subscription management that trap unwary developers.4.1 The Signature Generation ChallengePayFast requires a security signature for every transaction request. This signature is an MD5 hash of a concatenated string of variables. The variables must be:Stripped of empty parameters.Ordered alphabetically by key.URL-encoded (replacing spaces with +, not %20).Concatenated with the passphrase at the end.Python's urllib.parse.urlencode handles most of this, but quote_plus is specifically required to match the PHP-style encoding PayFast expects.34.2 Subscription Frequency and CancellationFor a monthly subscription, the frequency parameter must be set to 3.5 A common error is using 1 (Daily) or monthly (string), which causes API rejection.Furthermore, managing subscriptions via API (e.g., cancelling a user's plan) requires a PUT request to the specific resource URL. Most RESTful APIs use DELETE for this action, making this a common point of failure for integrations.6 The correct endpoint structure is:PUT https://api.payfast.co.za/subscriptions/{token}/cancel4.3 PayFast Service ImplementationThe PayFastService class handles these complexities, abstracting the signature generation and API calls.app/services/payfast_service.pyPythonimport hashlib
import urllib.parse
import httpx
import time
from typing import Dict, Any
from app.config import settings

class PayFastService:
    # Environment Handling
    BASE_URL = "https://api.payfast.co.za" if settings.PAYFAST_ENV == "production" else "https://api.payfast.co.za"
    
    @staticmethod
    def generate_signature(data: Dict[str, Any]) -> str:
        """
        Generates the MD5 signature required by PayFast.
        Critical: 
        1. Sort keys alphabetically.
        2. Filter empty values.
        3. URL Encode values (Spaces become +).
        4. Append passphrase.
        """
        # 1. Sort and Filter
        ordered_data = dict(sorted({k: v for k, v in data.items() if v is not None and v!= ""}.items()))
        
        # 2. Construct Query String
        query_string = ""
        for key, value in ordered_data.items():
            # Use quote_plus to ensure spaces -> +
            encoded_val = urllib.parse.quote_plus(str(value))
            query_string += f"{key}={encoded_val}&"
            
        # Remove trailing &
        query_string = query_string[:-1]
        
        # 3. Append Passphrase
        if settings.PAYFAST_PASSPHRASE:
            # Passphrase must also be URL encoded
            passphrase_encoded = urllib.parse.quote_plus(settings.PAYFAST_PASSPHRASE)
            query_string += f"&passphrase={passphrase_encoded}"
            
        # 4. Hash
        return hashlib.md5(query_string.encode('utf-8')).hexdigest()

    @staticmethod
    def generate_subscription_form(
        user_email: str, 
        item_name: str, 
        amount: float
    ) -> Dict[str, Any]:
        """
        Generates the payload for a Monthly Subscription (Frequency 3).
        This data is sent to the Frontend to build the hidden form.
        """
        data = {
            "merchant_id": settings.PAYFAST_MERCHANT_ID,
            "merchant_key": settings.PAYFAST_MERCHANT_KEY,
            "return_url": "https://gogga.app/payment/success",
            "cancel_url": "https://gogga.app/payment/cancel",
            "notify_url": "https://api.gogga.app/api/v1/payment/notify",
            
            # User Details
            "email_address": user_email,
            
            # Item Details
            "m_payment_id": f"sub_{int(time.time())}", # Unique internal ID
            "amount": f"{amount:.2f}",
            "item_name": item_name,
            
            # Subscription Specifics [5, 8]
            "subscription_type": "1",    # 1 = Subscription
            "billing_date": time.strftime('%Y-%m-%d'),
            "recurring_amount": f"{amount:.2f}",
            "frequency": "3",            # 3 = Monthly
            "cycles": "0"                # 0 = Indefinite
        }
        
        # Generate Signature
        data["signature"] = PayFastService.generate_signature(data)
        
        return data

    @staticmethod
    async def cancel_subscription(token: str) -> bool:
        """
        Cancels a subscription using the PUT method.
        """
        endpoint = f"/subscriptions/{token}/cancel"
        url = f"{PayFastService.BASE_URL}{endpoint}"
        
        timestamp = time.strftime('%Y-%m-%dT%H:%M:%S')
        
        # Payload for Signature Generation
        # Note: For API calls, headers are often part of the auth check.
        # We sign the specific auth variables required by PayFast API V1.
        auth_payload = {
            "merchant-id": settings.PAYFAST_MERCHANT_ID,
            "version": "v1",
            "timestamp": timestamp,
            "passphrase": settings.PAYFAST_PASSPHRASE 
        }
        
        # Generate signature (Passphrase is part of hash string but not header)
        signature = PayFastService.generate_signature(
            {k:v for k,v in auth_payload.items() if k!= 'passphrase'}
        )
        # Note: The above signature gen needs the passphrase appended to string,
        # which our generate_signature method does automatically if configured.
        
        headers = {
            "merchant-id": settings.PAYFAST_MERCHANT_ID,
            "version": "v1",
            "timestamp": timestamp,
            "signature": signature,
            "content-type": "application/json"
        }
        
        async with httpx.AsyncClient() as client:
            # Use 'testing=true' query param if in sandbox
            params = {"testing": "true"} if settings.PAYFAST_ENV == "sandbox" else {}
            
            try:
                # Explicit PUT request
                response = await client.put(url, headers=headers, params=params)
                
                if response.status_code == 200:
                    resp_data = response.json()
                    if resp_data.get('code') == 200 and resp_data.get('status') == 'success':
                        return True
                        
                print(f"PayFast Cancellation Failed: {response.text}")
                return False
            except Exception as e:
                print(f"Connection Error: {e}")
                return False
5. Frontend Implementation: Next.js and Voice InteractionThe GOGGA frontend is built using Next.js 14 with the App Router architecture. This ensures optimized rendering performance and SEO, while providing a seamless developer experience with React Server Components.5.1 Voice Capabilities in the African ContextVoice interaction is not a luxury in the South African context; it is an accessibility requirement. Many users speak languages with complex orthographies (like isiXhosa) and find typing tedious, preferring to send "Voice Notes" (as seen in WhatsApp usage patterns).GOGGA implements a browser-based audio recorder using the MediaRecorder API. The audio is captured as a Blob (typically audio/webm or audio/mp4 depending on the browser support) and sent to the backend. The backend would ideally pass this to a Speech-to-Text (STT) service (like Whisper), though for this reference code, we focus on the capture and transmission mechanism.5.2 Key Frontend Componentssrc/components/AudioRecorder.tsxTypeScript'use client';

import { useState, useRef } from 'react';
import { Mic, Square } from 'lucide-react'; 

interface AudioRecorderProps {
  onAudioReady: (audioBlob: Blob) => void;
}

export default function AudioRecorder({ onAudioReady }: AudioRecorderProps) {
  const = useState(false);
  const mediaRecorderRef = useRef<MediaRecorder | null>(null);
  const chunksRef = useRef<Blob>();

  const startRecording = async () => {
    try {
      // Request microphone access
      const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
      
      // Determine supported mimeType (Safari supports mp4/aac, Chrome webm/opus)
      const mimeType = MediaRecorder.isTypeSupported('audio/webm') 
       ? 'audio/webm' 
        : 'audio/mp4';

      mediaRecorderRef.current = new MediaRecorder(stream, { mimeType });
      chunksRef.current =;

      mediaRecorderRef.current.ondataavailable = (e) => {
        if (e.data.size > 0) {
          chunksRef.current.push(e.data);
        }
      };

      mediaRecorderRef.current.onstop = () => {
        // Create final Blob
        const blob = new Blob(chunksRef.current, { type: mimeType });
        onAudioReady(blob);
        chunksRef.current =;
        
        // Stop all tracks to release hardware resource
        stream.getTracks().forEach(track => track.stop());
      };

      mediaRecorderRef.current.start();
      setIsRecording(true);
      
    } catch (err) {
      console.error("Microphone Error:", err);
      alert("Please allow microphone access to talk to Gogga.");
    }
  };

  const stopRecording = () => {
    if (mediaRecorderRef.current && isRecording) {
      mediaRecorderRef.current.stop();
      setIsRecording(false);
    }
  };

  return (
    <div className="flex items-center justify-center">
      {!isRecording? (
        <button 
          onClick={startRecording}
          className="p-3 bg-green-600 rounded-full hover:bg-green-700 text-white shadow-lg transition-transform hover:scale-105"
          aria-label="Start Recording"
        >
          <Mic size={24} />
        </button>
      ) : (
        <button 
          onClick={stopRecording}
          className="p-3 bg-red-600 rounded-full hover:bg-red-700 text-white shadow-lg animate-pulse"
          aria-label="Stop Recording"
        >
          <Square size={24} />
        </button>
      )}
    </div>
  );
}
src/app/page.tsx (Main Chat Interface)This component manages the chat state, handles the optimistic UI updates (showing the user's message immediately), and communicates with the backend API.TypeScript'use client';

import { useState, useEffect, useRef } from 'react';
import AudioRecorder from '@/components/AudioRecorder';
import { Send, Bot, User } from 'lucide-react';
import axios from 'axios';

interface Message {
  role: 'user' | 'assistant';
  content: string;
  meta?: {
    cost_zar?: number;
    model?: string;
  };
}

export default function ChatPage() {
  const [messages, setMessages] = useState<Message>();
  const [input, setInput] = useState('');
  const [isLoading, setIsLoading] = useState(false);
  const scrollRef = useRef<HTMLDivElement>(null);

  // Auto-scroll to bottom
  useEffect(() => {
    if (scrollRef.current) {
      scrollRef.current.scrollIntoView({ behavior: 'smooth' });
    }
  }, [messages]);

  const sendMessage = async (text: string) => {
    if (!text.trim()) return;
    
    // Optimistic UI Update
    const userMsg: Message = { role: 'user', content: text };
    setMessages(prev => [...prev, userMsg]);
    setInput('');
    setIsLoading(true);

    try {
      // API Call to FastAPI Backend
      const response = await axios.post(`${process.env.NEXT_PUBLIC_API_URL}/api/v1/chat`, {
        message: text,
        user_id: "demo_user_123" // In production, this comes from Auth context
      });

      const data = response.data;
      
      const botMsg: Message = { 
        role: 'assistant', 
        content: data.response,
        meta: {
          cost_zar: data.meta.cost_zar,
          model: data.meta.model_used
        }
      };
      
      setMessages(prev => [...prev, botMsg]);
      
    } catch (error) {
      console.error("Chat Error:", error);
      setMessages(prev =>);
    } finally {
      setIsLoading(false);
    }
  };

  const handleAudio = (audioBlob: Blob) => {
    // Placeholder: Upload blob to backend
    console.log("Audio recorded:", audioBlob.size, "bytes");
    alert("Audio Captured! Transcribe endpoint not linked in this reference code.");
  };

  return (
    <div className="flex flex-col h-screen bg-slate-50">
      {/* Header */}
      <header className="bg-emerald-700 text-white p-4 shadow-md flex justify-between items-center">
        <h1 className="text-xl font-bold tracking-tight">GOGGA ðŸ‡¿ðŸ‡¦</h1>
        <div className="text-xs bg-emerald-800 px-2 py-1 rounded">Beta v1.0</div>
      </header>

      {/* Chat Area */}
      <div className="flex-1 overflow-y-auto p-4 space-y-4">
        {messages.length === 0 && (
          <div className="text-center text-gray-400 mt-20">
            <p className="text-lg">Sawubona! How can I help you today?</p>
            <p className="text-sm">I can help with legal questions, code, or just a chat.</p>
          </div>
        )}
        
        {messages.map((m, i) => (
          <div key={i} className={`flex ${m.role === 'user'? 'justify-end' : 'justify-start'}`}>
            <div className={`flex gap-2 max-w-[85%] ${m.role === 'user'? 'flex-row-reverse' : 'flex-row'}`}>
              <div className={`p-2 rounded-full h-fit ${m.role === 'user'? 'bg-emerald-600' : 'bg-gray-300'}`}>
                {m.role === 'user'? <User size={16} className="text-white"/> : <Bot size={16} className="text-gray-700"/>}
              </div>
              
              <div className={`p-3 rounded-2xl shadow-sm text-sm ${
                m.role === 'user' 
                 ? 'bg-emerald-600 text-white rounded-tr-none' 
                  : 'bg-white text-gray-800 rounded-tl-none border border-gray-200'
              }`}>
                {m.content}
                {/* Cost/Model Metadata Display (Optional Debugging) */}
                {m.meta && (
                  <div className="mt-2 pt-2 border-t border-gray-100 text-[10px] opacity-70 flex gap-2">
                    <span>Model: {m.meta.model}</span>
                    <span>Cost: R{m.meta.cost_zar?.toFixed(4)}</span>
                  </div>
                )}
              </div>
            </div>
          </div>
        ))}
        
        {isLoading && (
          <div className="flex justify-start">
             <div className="bg-gray-200 p-3 rounded-lg rounded-tl-none text-xs text-gray-500 animate-pulse">
               Gogga is thinking...
             </div>
          </div>
        )}
        <div ref={scrollRef} />
      </div>

      {/* Input Area */}
      <div className="p-4 bg-white border-t border-gray-200">
        <div className="max-w-4xl mx-auto flex items-center gap-3">
          <AudioRecorder onAudioReady={handleAudio} />
          
          <div className="flex-1 relative">
            <input 
              type="text" 
              value={input}
              onChange={(e) => setInput(e.target.value)}
              onKeyDown={(e) => e.key === 'Enter' && sendMessage(input)}
              className="w-full border border-gray-300 rounded-full py-3 px-5 focus:outline-none focus:ring-2 focus:ring-emerald-500 focus:border-transparent transition-all"
              placeholder="Type your message..."
              disabled={isLoading}
            />
          </div>
          
          <button 
            onClick={() => sendMessage(input)}
            disabled={!input.trim() |

| isLoading}
            className="p-3 bg-emerald-600 text-white rounded-full hover:bg-emerald-700 disabled:bg-gray-300 disabled:cursor-not-allowed transition-colors"
          >
            <Send size={20} />
          </button>
        </div>
      </div>
    </div>
  );
}
6. Infrastructure: Docker and Azure Container AppsThe deployment strategy targets Azure Container Apps (ACA). ACA is a serverless container service built on Kubernetes (KEDA, Dapr, Envoy) but abstracts the complexity of cluster management. It is ideal for GOGGA because it supports scaling to zero (cost saving) and HTTP scaling rules.6.1 Local Orchestration with Docker ComposeThe docker-compose.yml file allows developers to spin up the entire stack locally with a single command (docker-compose up). It handles networking between the frontend and backend.docker-compose.ymlYAMLversion: '3.8'

services:
  backend:
    build: 
      context:./gogga-backend
      dockerfile: Dockerfile
    container_name: gogga_api
    ports:
      - "8000:8000"
    environment:
      - CEREBRAS_API_KEY=${CEREBRAS_API_KEY}
      - PAYFAST_MERCHANT_ID=${PAYFAST_MERCHANT_ID}
      - PAYFAST_MERCHANT_KEY=${PAYFAST_MERCHANT_KEY}
      - PAYFAST_PASSPHRASE=${PAYFAST_PASSPHRASE}
      - PAYFAST_ENV=sandbox
      - DATABASE_URL=postgresql://gogga:gogga@db:5432/gogga_db
    depends_on:
      - db
    volumes:
      -./gogga-backend:/app

  frontend:
    build:
      context:./gogga-frontend
      dockerfile: Dockerfile
    container_name: gogga_ui
    ports:
      - "3000:3000"
    environment:
      - NEXT_PUBLIC_API_URL=http://localhost:8000
    depends_on:
      - backend

  db:
    image: postgres:15-alpine
    environment:
      - POSTGRES_USER=gogga
      - POSTGRES_PASSWORD=gogga
      - POSTGRES_DB=gogga_db
    ports:
      - "5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data

volumes:
  postgres_data:
6.2 Deployment to AzureFor the production environment, we select the South Africa North (Johannesburg) region. This is critical for data sovereignty compliance (POPIA) and ensuring low latency for local users.Dockerfile (Backend Optimization)We use a multi-stage build to keep the image size small, reducing cold-start times in Azure Container Apps.Dockerfile# Stage 1: Builder
FROM python:3.11-slim as builder
WORKDIR /app
COPY requirements.txt.
RUN pip install --user --no-cache-dir -r requirements.txt

# Stage 2: Runtime
FROM python:3.11-slim
WORKDIR /app
# Copy installed packages from builder
COPY --from=builder /root/.local /root/.local
COPY..
ENV PATH=/root/.local/bin:$PATH

CMD ["uvicorn", "app.main:app", "--host", "0.0.0.0", "--port", "8000"]
Azure CLI Deployment CommandsThese commands provision the environment. Note the use of target-port and ingress settings.Bash# 1. Login and Set Context
az login
az account set --subscription "<YOUR_SUBSCRIPTION_ID>"

# 2. Create Resource Group in SA North
az group create --name GoggaRG --location southafricanorth

# 3. Create Azure Container Registry (ACR)
az acr create --resource-group GoggaRG --name goggaregistry --sku Basic

# 4. Build Images in the Cloud (Avoids local upload bandwidth issues)
az acr build --registry goggaregistry --image gogga-backend:v1./gogga-backend
az acr build --registry goggaregistry --image gogga-frontend:v1./gogga-frontend

# 5. Create Container Apps Environment
az containerapp env create \
  --name gogga-env \
  --resource-group GoggaRG \
  --location southafricanorth

# 6. Deploy Backend (Secrets injection)
az containerapp create \
  --name gogga-backend \
  --resource-group GoggaRG \
  --environment gogga-env \
  --image goggaregistry.azurecr.io/gogga-backend:v1 \
  --target-port 8000 \
  --ingress 'external' \
  --min-replicas 1 \
  --max-replicas 10 \
  --secrets cerebras-key=<KEY> payfast-pass=<PASS> \
  --env-vars CEREBRAS_API_KEY=secretref:cerebras-key PAYFAST_PASSPHRASE=secretref:payfast-pass

# 7. Deploy Frontend
# Note: Get the backend URL from the previous step output
az containerapp create \
  --name gogga-frontend \
  --resource-group GoggaRG \
  --environment gogga-env \
  --image goggaregistry.azurecr.io/gogga-frontend:v1 \
  --target-port 3000 \
  --ingress 'external' \
  --env-vars NEXT_PUBLIC_API_URL=https://gogga-backend.<UNIQUE_ID>.southafricanorth.azurecontainerapps.io
7. Implications and Future Roadmap7.1 Scalability and Cost AnalysisThe Bicameral architecture effectively insulates the business model from linear cost scaling. By deflecting 70-80% of routine traffic to the Cerebras Speed Layer ($0.10/M tokens), the average cost per user interaction remains below $0.0005. The heavy lifting is reserved for high-value queries where the Qwen 3 model's cost ($0.60/M tokens) is justified by the utility provided.7.2 POPIA ComplianceThe architecture supports POPIA (Protection of Personal Information Act) compliance by keeping all infrastructure within the southafricanorth Azure region. While the inference call to Cerebras travels to US data centers (currently), the transient nature of the request and the absence of storage on the inference side (stateless API) mitigates data residency risks, provided clear user consent is obtained.7.3 ConclusionProject GOGGA demonstrates that building a world-class AI application for the African market requires more than just wrapping an OpenAI endpoint. It demands a deliberate architectural strategy that accounts for currency volatility, linguistic diversity, and bandwidth constraints. By combining the raw speed of Cerebras, the deep reasoning of Qwen 3, and the robustness of Azure and PayFast, GOGGA stands as a blueprint for the next generation of sovereign African software.
